## 機械学習 レポート

# 【学習の要点】
    修了テストの受験を通して、以下3テーマに関するアウトプットを行った。 1.線形代数基礎 2.統計学基礎 3.情報理論基礎

# 【まとめ】
    機械学習・ディープラーニング分野におけるデータを基にした「予測・分類」に必要であることを念頭に、学習した。
    それぞれの分野は重なったり延長線上にあることもあるため、体系的な学習を行う必要がある。
    以下各テーマ要旨において、学習内容とその必要性をまとめた。

# 【各テーマの要旨】
    1.線形代数基礎
        基礎的なベクトル・行列演算から、固有値分解・特異値分解にかかる大学基礎レベルの線形代数理論を学習した。
        線形代数は ベクトル・行列の計算は4次元以上のデータ解析において計算結果を抽象的に表すことができる点で非常に優れている。
        この点は、機械学習・ディープラーニング分野において例えば画像解析などにおいて効果を発揮すると思われる。
        なぜならば、画像は各画素にバイナリ値を持っており、画像が細かくなればなるほど画素は増え、画像の解析を行う際膨大な変数
        (画素の数)を扱わなければならないためだ。
        実際、マシンにfor文等繰り返し演算させるのと、行列として演算させるのでは演算速度に大きな差があり行列演算が圧倒的に速くなるので、
        線形代数の基礎を学び、プログラミングに行列演算を落とし込むことができればレスポンスの早いプログラムを書くことができる。

    2.統計学基礎
        基本的なベイズ確率理論から、基本的な確率分布のパターンと期待値・分散の高校レベルの考え方を学習した。
        離散確率変数と連続確率変数の違い、各分布に従ったときの期待値・分散の求め方を中心に学習をした。
        機械学習・ディープラーニングの一つの目的は、ビッグデータを基にしてマシンにそのビッグデータから「予測・分類」させることだと思う。
        予測・分類のための理論として、例えば最小二乗法を利用した予測、あるいはロジスティック回帰を利用した分類などがあげられるが、
        それら理論は統計学の「回帰」の考え方を利用しているため、上記の基礎統計理論を学習することが不可欠だ。

    3.情報理論基礎
        自己情報量・シャノンエントロピーの概念を重点的に学習した。
        情報理論で学んだのは、ある事象が起きた時に知りうる情報量を数学的に表現するという考え方であり、ベイズ確率理論の延長線上にある考え方だと思われる。
        自己情報量では、事象が起こる確率が高ければ知ることになる情報量は少なく(当たり前の事象),
        確率が低ければ情報量は多い(珍事)という現象を表現している。
        平均情報量という期待値によって、当たり前と珍事の線引きとして数学的に表現しているように思われた。
        不確かな情報を扱う「予測・分類」の分野にとって、各情報のあいまいさを表現する手段として、エントロピーの考え方を学んだ。
