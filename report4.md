# ディープラーニング Day2(ラビットチャレンジ)

# Day2
# 本日のテーマ　＝　DLの学習テクニック

誤差逆伝播法のデメリット　＝　勾配消失問題<br>
- 下位層に進むにつれて勾配が緩やかとなり、勾配降下法による最適値収束がなされない

#### Section1) 勾配消失問題 
勾配消失問題を防ぐための方策<br>
- 1. 活性化関数
 - ReLU関数の導入
  - →シグモイド関数の場合、勾配消失が起こる可能性がある
- 2. 初期値の設定方法
 - Xavierの初期値(シグモイド関数・ReLU関数・tanh関数に有効)
  - 重みの要素を前の層のノード数の平方根で除算する
 - Heの初期値(ReLU関数に有効)
  - 重みの要素を前の層のノードの平方根で除算した値に対し√2をかける
- 3. バッチ正規化
  - ミニバッチ単位で入力値のデータの偏りを抑制する
  - 活性化関数に値を渡す前に、バッチ正規化処理を含んだ層を加える
  - [![Image](https://gyazo.com/bd6f1c0bea11ec1c14173ff049b1cb61/thumb/1000)](https://gyazo.com/bd6f1c0bea11ec1c14173ff049b1cb61)
  - [![Image](https://gyazo.com/b3e1f4eff546f499a9b36664a4c8e345/thumb/1000)](https://gyazo.com/b3e1f4eff546f499a9b36664a4c8e345)

#### Section2) 学習率最適化手法 
- パラメータは目的関数最適化問題で最適化できる
- パラメータを最適化しても学習率が最適でないと、発散したり時間がかかる
- 手計算で、学習率を徐々に小さくしたりすることは可能
- 学習率の最適化手法は？？　→　4つの方法がある
- 1. モメンタム
 - [![Image](https://gyazo.com/4de7a1ca6114cd23140e3017bd951011/thumb/1000)](https://gyazo.com/4de7a1ca6114cd23140e3017bd951011)
- 2. AdaGrad
 - [![Image](https://gyazo.com/f1fe6ced837fe54d4e309b5aa51d653d/thumb/1000)](https://gyazo.com/f1fe6ced837fe54d4e309b5aa51d653d)

- 3. RMSProp
 - [![Image](https://gyazo.com/b6f5ad54b224e8f56cadfd068b08650b/thumb/1000)](https://gyazo.com/b6f5ad54b224e8f56cadfd068b08650b)
- 4. Adam
 - [![Image](https://gyazo.com/79caa5f633ed48a854ff121e81e133d1/thumb/1000)](https://gyazo.com/79caa5f633ed48a854ff121e81e133d1)

#### Section3) 過学習 
- 訓練誤差とテスト誤差で学習された値が近似してしまうと汎化性能は疑問
- 荷重減衰(Weight Decay) : 大きな重みを持たせることで過学習発生の可能性がある
- 過学習を防ぐための方策
- 1. L1, L2正則化
 - ネットワークの自由度を制約することで過学習を抑制する
 - 誤差に正則化項を追加し、重みの影響を抑制する
 - 過学習が起きそうな重み以下で、かつ重みの大きさをばらつかせる
- 2. ドロップアウト
 - ノード数が多い場合、ランダムにノードを削除して過学習を抑制する
#### Section4) 畳み込みニューラルネットワーク(CNN) 
- CNNの全体構造
- [![Image](https://gyazo.com/0a6f743bdf1d2b752aaeadb075724265/thumb/1000)](https://gyazo.com/0a6f743bdf1d2b752aaeadb075724265)

- 畳み込み層
 - 3次元の空間情報をも学習することができる層
 - 全結合層では3次元データは一度1次元のデータとして反映されてしまう
 - 各チャンネルの関連性が学習に反映されない
 - 入力データにフィルターとバイアスをかけ、中間層により圧縮したデータを渡す
  - パディング
   - フィルターの粗さ。より大きくすることで出力データは大きくなるが保持する情報も多くなる
  - ストライド
   - どれくらいの範囲でフィルターを入力データにかけるか
- プーリング層
 - 入力データに存在する要素のMax値/平均値を取得して出力する
 - 畳みこんできたデータを要約して後ろの層に伝える役割を持つ
#### Section5) 最新のCNN 
- AlexNet
 - DLが注目されるきっかけとなったCNNモデル
 - 5層の畳み込み層・プーリング層と、3層の全結合層から構成される
 - 過学習抑制にドロップアウトを使用
