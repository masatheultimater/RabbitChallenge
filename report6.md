# ディープラーニング Day4(ラビットチャレンジ)

# Day4
# 本日のテーマ　＝　FWによるDL実装/強化学習

### Section1) TensorFlow
- DL実装においてはTensorFlowによる実装のシェアが圧倒的
- そのため、今回はTensorFlowでの実装を学習する
 - 基本演算
  - Session：Sessionを定義したのち、Runすることでインスタンスを生成し、データを参照・操作することができる
  - PlaceHolder：Sessionに用意するFW変数。のちにPlaceHolderの値を変更することができる
   - 例　バッチの大きさを変更したりするときに使用
  - Variables：変数を収納するFW変数
 - 機械学習の実装
  - 線形回帰
   - エポック・誤差表示を設定
   - データをインポート　あるいはランダム生成(+ノイズ)
   - PlaceHolderにデータを代入
   - Variablesに目的変数・重みを代入
   - FWに用意されているMSEの式を生成
   - SessionをRunして、MSEにVariablesを渡して学習
   - FWに用意されているPredict関数で予測
   - VariablesやPlaceholderの値を変更することで容易にモデルの変更が可能
    - 例　どのような最適化方法や正則化を行うかなど関数を選択することで容易に変更できる
  - 非線形回帰
   - 特徴量ごとのPlaceHolderを用意する
   - optimizerをadamOptimizerに変更
   - 予測部分も重み分だけ変更
  - ポイント
   - レイヤー数やユニット数、重みの初期値などは試行錯誤してチューニングしていかなければならないように感じた
 - MNISTの実装
  - 1層NN分類の実装
   - MNISTデータをインポート
   - x, d, W, b をそれぞれPlacHolderを利用して初期化
   - yはFWのsoftmax関数を利用して実装
   - 誤差関数はクロスエントロピーを利用する
   - 最適化はGradientDescentOptimizerを利用
   - ミニバッチ勾配でfor文
   - 正解データと照らして正解なcorrectに加算し、後正答率を算出する
  - 3層分類の実装
   - hiddenLayerをPlaceHolderで2つ追加で用意する
   - 784*600*400*10の3層構造
   - W1,W2,W3,b1,b2,b3それぞれVariableで用意
   - 足し合わせにはmatmulを利用する
   - ドロップアウトを利用して勾配消失を考慮する
    - ドロップアウト率を用意してfeed_dictに渡し、可変化させる
   - ポイント
    - 隠れ層のユニット数を減らすと学習はスムーズだが精度は落ちる
    - 隠れ層のユニット数を減らすと学習は遅いが精度は上がる
    - 中間層のサイズは精度と学習速度のトレードオフになっている
    - 勾配降下法の最適化法の変更
     - Adam
     - GradientDescent
     - Momentum
     - AdaGrad
     - RMSProp　正解率が一番上がった 3000回試行
      - 精度が頭打ちになった段階で比較すべき
  - CNNの実装
   - 構造：Conv * pool * Conv * pool * affine * dropout * affine
   - フィルター, padding, strideなどをFW関数を利用することで容易に用意できる
   - affineの用意　matmul, reluで計算
   - 最後にsoftmaxで出力
   - dropout_ratioを変更すると同じ試行回数でも
    - 落とすと精度は上がるが学習速度は下がる
    - 上げると精度は落ちるが学習速度は上がる
   - 同じ試行回数では、3層NNよりも精度が上がった
   - テストデータの精度はドロップアウトがあるほうが上がる可能性がある
 - Kerasによる実装
  - TensorFlowのラッパーライブラリ
  - TensorFlowより容易にDL、MLを行うことができる
  - 特徴
   - PlaceHolder、variableを用意しなくてよい
   - お手軽なモデルの型が用意されていて、モデルを選択し、データを投入するだけで学習・予測することができる
   - ほぼブラックボックスのため、凝った実装をすることができないため、簡単な実験に使うだけになりそう？
### section2) 最近のNN
- GoogLeNet
 - inception moduleの積み重ねで構成されている
 - フィルターサイズを定義し、複数のフィルターサイズを使用することでconvolutionを行う
 - 1*1の畳み込みを行い次元削減を行うことで計算量の低減
 - 複数の畳み込みを行ってパラメータを削減し表現力を上げる
 - 必要なパラメータは増えるがその分スパースな演算となる
 - Loss　途中のNWから分岐させたサブNWでクラス分類を行っている点が特徴
- ResNet
 - Facebookが作ったモデル
 - 最近も汎用的に使われている
 - 層の深さが特徴　152層
  - 深くすることで表現力は上がるが…
  - 勾配消失問題があるため、既存モデルでは深くできなかった。浅いモデルより訓練誤差が大きくなっていた。
 - 層をまたがる結合があるIdentity mapping スキップコネクション　→　そのまま写像　→　勾配消失を解決
 - ResNetブロック
 - 残差ブロックでは畳み込み層とskip connectionの組み合わせ
- 転移学習
 - ほかのタスクで使用したモデルを利用する
  - ワンショット学習　ある画像に対してベクトルを用意する
   - それに対して近いか遠いかで学習をさせる
- 一般物体検出アルゴリズム
 - YOLO
  - シンプルな分検出速度は高速だが、画像内オブジェクトが重なっている場合うまく検出されないことがある
  - 必ず2つの候補領域が特定される
  - 2つの候補領域とそれぞれの＋の確率が出力される
 - Single shot Multibox detector
  - 誤差の和は、重みづけされた誤差の和で表現される
 - 

### section2) 強化学習
 - 強化学習とは
  - 長期的な報酬が最大化される行動を選択できるエージェントを作る機械学習
  - 行動結果の報酬を基にして行動を決定するアルゴリズム
 - 探索と利用のトレードオフ
  - 環境について完全情報の場合　→最適行動はすぐ決定できる
  - 環境について不完全情報の場合→行動しながらデータ収集し、最適化
  - 一番よかった行動のみをとり続けると→機会損失（探索不足）
  - 未知の行動のみをとり続けると　　　→経験が活きない（利用不足）
 - 強化学習のイメージ
  - 方策　　　π\piπ
  - 環境状態　SSS
  - 価値　　　VVV
  - 方策関数π(S,a)\pi(S, a)π(S,a)
  - 行動価値関数　Q(S,a)Q(S, a)Q(S,a)
 - 強化学習の差分
  - 強化学習の目的：エージェントが最適行動となる優れた方策を見つける
  - 教師あり・なし学習の目的：データに含まれるパターンを見つけ、予測する
 - 最近の強化学習
  - Q学習と関数近似法の組み合わせにより進歩
 - 行動価値関数
  - 状態価値関数　状態の価値
  - 行動価値関数　状態の価値＋行動の価値
 - 方策関数
  - 方策ベースの強化学習手法で、ある行動を選択する確率を与える関数
 - 方策勾配法
  - 方策反復法　モデル化された方策を勾配法を用いて最適化する
[![Image](https://gyazo.com/33f7010c8b0ee3c427c8db0371e23cc0/thumb/1000)](https://gyazo.com/33f7010c8b0ee3c427c8db0371e23cc0)<br>

  - ある方策をとったときの「良さJ(θ)J(\theta)J(θ)」の傾きが0に収束するまで反復してシミュレーションする
  - 「良さ」の定義
   - 平均報酬
    - 時間割引考慮無し？
   - 割引報酬和
    - 時間割引？
方策勾配定理<br>
[![Image](https://gyazo.com/89de4a7d6b5bf14683814d92075b33bb/thumb/1000)](https://gyazo.com/89de4a7d6b5bf14683814d92075b33bb)<br>
[![Image](https://gyazo.com/c87629c50ee3eefdecb3cfa70632e22d/thumb/1000)](https://gyazo.com/c87629c50ee3eefdecb3cfa70632e22d)<br>
