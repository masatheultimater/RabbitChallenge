[#Day1](https://scrapbox.io/mysnowsmile-29610799/Day1)<br>
 # 深層学習がやりたいこと
 - 誤差を最小化するパラメータを発見すること
  - パラメータ＝重みとバイアス
  - 誤差E(w)E(w)E(w)を最小化するパラメータwwwを発見すること
 - 仕組み
  - 事前に用意する情報
   - 入力データ x　入力用
   - 訓練データ d　検証用
  - パラメータ
   - w = 重み(W) , バイアス(b)

  - 出力
   - 活性化関数 f(u)
   - 中間層出力 z = [zi - zk](https://scrapbox.io/mysnowsmile-29610799/zi_-_zk) = f(u)
   - 総出力 u = Wz + b
   - 出力 yn = z
   - 誤差関数 En(w)

# ニューラルネットワークでできること
1. 回帰
  - 結果予測
   - 売上予測　→　価格最適化、マーケット最適化
   - 株価予測
  - ランキング
   - 競馬順位予想
   - 人気順位予想
2. 分類
  - 猫写真の判別
  - 手書き文字認識
  - 花の種類分類　アイリスデータ
  - 保険証の読み取り
  - 車のナンバー認識
 - 次元数が増えたら増えただけ、学習にうまみがある　⇔ルールベースPGM
3. 連続する実数地をとる関数の近似
4. 離散的な結果を予想するための分析
 - ニューラルネットワークはこちらに属している
- 深層学習の実用例
 - 自動売買(トレード)
 - チャットボット　RNN
 - 翻訳　RNN
 - 音声解釈　Amazon Echo, Google Home, Dragon Speach
>精度の高い音声データを持つことは難しい。例えばキャリアなどは優位<br>
>個人の場合、既存のライブラリ・API(Google)を作り変えてやることが多い<br>
>精度を上げるには、例えば圧縮方法を変えることで精度を上げたりできる<br>
 - 囲碁将棋AI　CNN　　　　　　など
>強化学習とCNNのハイブリッドなど<br>

>いいものは外には出ない。基礎を学んで自らひらめきを実装できる力をつける<br>

# Section 1)入力層～中間層<br>
1. 入力層での入力<br>
入力ベクトルx に重み行列W、バイアスbを混ぜ、総入力uを得て、中間層に渡す　u= Wx + b<br>
2. 中間層<br>
入力層から得た総出力uを活性化関数に入力し、出力zを得る　z = f(u)<br>

# Section 2)活性化関数<br>
- 定義
  - f(l)(ul)=[f(l)(u1(l))...f(l)(uj(l))]f^{(l)}(u^{l}) = [f^{(l)}(u^{(l)}_1) ... f^{(l)}(u^{(l)}_j)\rbrack f(l)(ul)=[f(l)(u1(l)​)...f(l)(uj(l)​)]
   - 入力層ノードi, 中間層ノード j, 層のインデックスl
   - ##### NNにおいて、次の層への出力の大きさを決める非線形の関数
    - 入力の値によって、次の層への信号のON/OFFや強弱を決める
- 中間層の活性化関数
 - ステップ関数
 - f(x)={1(x>0)0(x≤0)f(x)=\left\{ \begin{array}{l} 1 & (x > 0)\\0 & (x \leq 0) \end{array} \right.f(x)={10​(x>0)(x≤0)​
  - 出力は常に0,1
  - 閾値を超えると発火する
  - パーセプトロンで利用された関数
   - ##### 課題　0,1間の間を表現できない機械的表現関数

 - シグモイド関数
  - f(x)=11+e−xf(x) = \dfrac{1}{1 + e^{-x}}f(x)=1+e−x1​
  - ０～１間を緩やかに変化する
  - 結果、0,1はとらない（微小な値を与え続ける）
  - 状態に対して、信号の強弱を伝えられるようになった
  - 予想NN普及のきっかけになった
   - 課題　##### 大きな値では出力の変化が微小！
    - ##### 　→勾配消失問題を引き起こす可能性
    - ##### 　計算にリソースを食いやすい
 - ReLU関数
 - f(x)={x(x>0)0(x≤0)f(x) = \left\{\begin{array}{l} x & (x > 0) \\ 0 & (x \leq 0) \end{array}\right. f(x)={x0​(x>0)(x≤0)​
  - 最も使われている活性化関数
  - ##### 勾配消失問題の回避とスパース化に貢献

- 出力層用の活性化関数　→　ソフトマックス関数・恒等写像・シグモイド関数

# Section 3)出力層
- 出力　　：yn(t)=[yn1(t)...ynk(t)]=z(L)y^{(t)}_{n} = [y^{(t)}_{n1} ... y^{(t)}_{nk}] = z^{(L)} yn(t)​=[yn1(t)​...ynk(t)​]=z(L)
- 誤差関数：E(w)E(w)E(w)　例：二乗誤差　E(w)=12∑j=1J(yj−dj)2=12∣∣(y−d)∣∣2E(w) = \frac{1}{2} \sum_{j=1}^{J}(y_j - d_j)^2 = \frac{1}{2} || (y - d)||^2E(w)=21​j=1∑J​(yj​−dj​)2=21​∣∣(y−d)∣∣2
   - 出力層ノードK, 層のインデックスL, 試行回数のインデックスT, 訓練データのインデックスN
   - 出力＝t回目の試行の各出力層ノード出力のベクトル＝出力層ノードの中間層出力
>>見せ方が重要となるため工夫が必要<br>
 - 「出力　⇔　訓練データ」　を比較し、　「誤差関数」　→　出力の確からしさを定量的に評価する
 - ##### NNのゴール　＝　学習させていくにつれて誤差を小さくする(精度を上げる)こと
 - ### 出力層の特徴
  - 人間が知覚するスカラー量として算出することができる
  - 出力層は信号の大きさ(比率)はそのままに変換したい
   - ##### →中間層の活性化関数とは異なる
 - 出力層の活性化関数
[![Image](https://gyazo.com/b0b8355f81dc2784f08fb69d3fbbbbc2/thumb/1000)](https://gyazo.com/b0b8355f81dc2784f08fb69d3fbbbbc2)<br>

# Section 4)勾配降下法

- 誤差を最小化するという目的を達成するための最適化法の一つ
- 出力ノードや特徴量、中間ノードが多すぎると計算量が多くなる
 - →確率的勾配降下法を使う
- 勾配降下法の学習率の決定アルゴリズム
 - ①Momentum
 - ②AdaGrad
 - ③Adadelta
 - ④Adam
- 確率的勾配降下法
 - 全サンプル集合からランダムに抽出した誤差を使用する
[![Image](https://gyazo.com/4cfdaab9db8fafd709adab6a4cc9dc9a/thumb/1000)](https://gyazo.com/4cfdaab9db8fafd709adab6a4cc9dc9a)<br>
- ミニバッチ勾配降下法
 - ランダムに分割したデータの集合サンプルの平均誤差を使用する
  - →SGDで、よりリソースをセーブできる
[![Image](https://gyazo.com/3338e092fec8adffa63d5ec32aa33fd2/thumb/1000)](https://gyazo.com/3338e092fec8adffa63d5ec32aa33fd2)<br>
- DtD_tDt​: データ集合
- NtN_tNt​:データ集合内データ数
- EtE_tEt​:データ誤差
- 誤差逆伝播法
 - 繰り返し順伝播計算ではない勾配計算
 - 数値微分を利用する

# Section 5)誤差逆伝播法
#### 特徴
- SGDで課題だった計算リソースの有効利用を目的にしたパラメータ最適化法
- 算出された誤差を、出力層側から順に微分し、前の層へと伝播させる
- 最小限の計算で各パラメータでの微分値を解析的に計算する
[![Image](https://gyazo.com/1ec6664a803474b61836c8f98f64014a/thumb/1000)](https://gyazo.com/1ec6664a803474b61836c8f98f64014a)<br>

[![Image](https://gyazo.com/9db78cd62535f8fda2c5aec516353f3c/thumb/1000)](https://gyazo.com/9db78cd62535f8fda2c5aec516353f3c)<br>
[![Image](https://gyazo.com/a0c7537c80c8ca08fb419702714d0df4/thumb/1000)](https://gyazo.com/a0c7537c80c8ca08fb419702714d0df4)<br>
[![Image](https://gyazo.com/fb05843d6d719c3f43c1a956ab6faed8/thumb/1000)](https://gyazo.com/fb05843d6d719c3f43c1a956ab6faed8)<br>

# 修了課題
ソースコード<br>
DLの構造図・可視化図(分類/回帰どちらにしたか、入力・中間・出力ノードと活性化関数・誤差関数の明示化)<br>
